{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZZP8Rbb45dF",
        "outputId": "9c2d3a45-1624-47c9-a9bf-b145026e0603"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-17 17:47:35--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.2’\n",
            "\n",
            "input.txt.2         100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-12-17 17:47:35 (20.3 MB/s) - ‘input.txt.2’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MyMultiHeadAttention(nn.Module):\n",
        "    def __init__(self, my_d_model, my_n_heads):\n",
        "        super(MyMultiHeadAttention, self).__init__()\n",
        "        self.my_n_heads = my_n_heads\n",
        "        self.my_d_head = my_d_model // my_n_heads\n",
        "\n",
        "        self.W_q = nn.Linear(my_d_model, my_d_model)\n",
        "        self.W_k = nn.Linear(my_d_model, my_d_model)\n",
        "        self.W_v = nn.Linear(my_d_model, my_d_model)\n",
        "        self.W_o = nn.Linear(my_d_model, my_d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        Q = self.W_q(x)\n",
        "        K = self.W_k(x)\n",
        "        V = self.W_v(x)\n",
        "\n",
        "        Q = Q.view(Q.size(0), -1, self.my_n_heads, self.my_d_head).transpose(1, 2)\n",
        "        K = K.view(K.size(0), -1, self.my_n_heads, self.my_d_head).transpose(1, 2)\n",
        "        V = V.view(V.size(0), -1, self.my_n_heads, self.my_d_head).transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.my_d_head ** 0.5)\n",
        "        attn_weights = nn.functional.softmax(scores, dim=-1)\n",
        "\n",
        "        out = torch.matmul(attn_weights, V).transpose(1, 2).contiguous()\n",
        "        out = out.view(out.size(0), -1, self.my_n_heads * self.my_d_head)\n",
        "\n",
        "        return self.W_o(out)"
      ],
      "metadata": {
        "id": "iIeUKi5V5Atz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyFeedForward(nn.Module):\n",
        "    def __init__(self, my_d_model, my_d_ff):\n",
        "        super(MyFeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(my_d_model, my_d_ff)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.linear2 = nn.Linear(my_d_ff, my_d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.functional.relu(self.linear1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "5ixKp9qu5GqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyTransformerBlock(nn.Module):\n",
        "    def __init__(self, my_d_model, my_n_heads, my_d_ff):\n",
        "        super(MyTransformerBlock, self).__init__()\n",
        "        self.self_attention = MyMultiHeadAttention(my_d_model, my_n_heads)\n",
        "        self.feed_forward = MyFeedForward(my_d_model, my_d_ff)\n",
        "        self.layer_norm1 = nn.LayerNorm(my_d_model)\n",
        "        self.layer_norm2 = nn.LayerNorm(my_d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_output = self.self_attention(x)\n",
        "        x = x + attn_output\n",
        "        x = self.layer_norm1(x)\n",
        "\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = x + ff_output\n",
        "        x = self.layer_norm2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "lgOtGvBc5JVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyGPT2Small(nn.Module):\n",
        "    def __init__(self, my_vocab_size, my_d_model=768, my_n_heads=12, my_n_layers=12):\n",
        "        super(MyGPT2Small, self).__init__()\n",
        "        self.my_d_model = my_d_model\n",
        "        self.my_n_heads = my_n_heads\n",
        "        self.my_n_layers = my_n_layers\n",
        "        self.my_vocab_size = my_vocab_size\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(my_vocab_size, my_n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(my_block_size, my_n_embd)\n",
        "        self.transformer_blocks = nn.Sequential(*[MyTransformerBlock(my_d_model, my_n_heads, 512) for _ in range(my_n_layers)])\n",
        "        self.linear = nn.Linear(my_d_model, my_vocab_size)\n",
        "\n",
        "    def forward(self, my_idx, targets=None):\n",
        "        B, T = my_idx.shape\n",
        "        tok_emb = self.token_embedding_table(my_idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=my_device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.transformer_blocks(x)\n",
        "        logits = self.linear(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = nn.functional.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, my_idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            my_idx_cond = my_idx[:, -my_block_size:]\n",
        "            logits, loss = self(my_idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = nn.functional.softmax(logits, dim=-1)\n",
        "            my_idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            my_idx = torch.cat((my_idx, my_idx_next), dim=1)\n",
        "        return my_idx"
      ],
      "metadata": {
        "id": "UVptoBbg5NJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_batch_size = 16\n",
        "my_block_size = 32\n",
        "my_max_iters = 20\n",
        "my_eval_interval = 100\n",
        "my_learning_rate = 1e-3\n",
        "my_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "my_eval_iters = 200\n",
        "my_n_embd = 464\n",
        "my_n_head = 16\n",
        "my_n_layer = 48\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as my_f:\n",
        "    my_text = my_f.read()\n",
        "\n",
        "my_chars = sorted(list(set(my_text)))\n",
        "my_vocab_size = len(my_chars)\n",
        "my_stoi = {ch: i for i, ch in enumerate(my_chars)}\n",
        "my_itos = {i: ch for i, ch in enumerate(my_chars)}\n",
        "my_encode = lambda s: [my_stoi[c] for c in s]\n",
        "my_decode = lambda l: ''.join([my_itos[i] for i in l])\n",
        "\n",
        "my_data = torch.tensor(my_encode(my_text), dtype=torch.long)\n",
        "my_n = int(0.9 * len(my_data))\n",
        "my_train_data = my_data[:my_n]\n",
        "my_val_data = my_data[my_n:]\n",
        "\n",
        "def get_my_batch(split):\n",
        "    my_data = my_train_data if split == 'train' else my_val_data\n",
        "    ix = torch.randint(len(my_data) - my_block_size, (my_batch_size,))\n",
        "    x = torch.stack([my_data[i:i + my_block_size] for i in ix])\n",
        "    y = torch.stack([my_data[i + 1:i + my_block_size + 1] for i in ix])\n",
        "    x, y = x.to(my_device), y.to(my_device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_my_loss():\n",
        "    out = {}\n",
        "    losses = {}\n",
        "    for split in ['train', 'val']:\n",
        "        for k in range(my_eval_iters):\n",
        "            x, y = get_my_batch(split)\n",
        "\n",
        "            logits, loss = my_model(x, y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = sum(losses.values()) / len(losses)\n",
        "    return out"
      ],
      "metadata": {
        "id": "BcTqcLQh5SV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "my_save_path = 'my_gpt2_task1.pth'\n",
        "\n",
        "if os.path.isfile(my_save_path):\n",
        "    my_model = MyGPT2Small(my_vocab_size, my_n_embd, my_n_head, my_n_layer)\n",
        "    my_model.load_state_dict(torch.load(my_save_path))\n",
        "    my_model = my_model.to(my_device)\n",
        "    my_model.eval()\n",
        "    print(f\"My Model loaded from {my_save_path}\")\n",
        "    print(sum(p.numel() for p in my_model.parameters()) / 1e6, 'M parameters')\n",
        "else:\n",
        "    my_model = MyGPT2Small(my_vocab_size, my_n_embd, my_n_head, my_n_layer)\n",
        "    my_model = my_model.to(my_device)\n",
        "    print(sum(p.numel() for p in my_model.parameters()) / 1e6, 'M parameters')\n",
        "\n",
        "    my_optimizer = torch.optim.AdamW(my_model.parameters(), lr=my_learning_rate)\n",
        "\n",
        "    for my_iter in range(my_max_iters):\n",
        "        if my_iter % my_eval_interval == 0 or my_iter == my_max_iters - 1:\n",
        "            my_losses = estimate_my_loss()\n",
        "            print(f\"step {my_iter}: train loss {my_losses['train']:.4f}, val loss {my_losses['val']:.4f}\")\n",
        "\n",
        "        my_xb, my_yb = get_my_batch('train')\n",
        "\n",
        "        my_logits, my_loss = my_model(my_xb, my_yb)\n",
        "        my_optimizer.zero_grad(set_to_none=True)\n",
        "        my_loss.backward()\n",
        "        my_optimizer.step()\n",
        "\n",
        "    torch.save(my_model.state_dict(), my_save_path)\n",
        "    print(f\"My Model saved to {my_save_path}\")\n",
        "\n",
        "my_model.eval()\n",
        "\n",
        "my_context = torch.zeros((1, 1), dtype=torch.long, device=my_device)\n",
        "my_generated_text = my_decode(my_model.generate(my_context, max_new_tokens=2000)[0].tolist())\n",
        "print(my_generated_text)"
      ],
      "metadata": {
        "id": "mBI71e_M5Wd5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}