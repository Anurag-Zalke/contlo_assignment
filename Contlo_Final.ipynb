{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZUSUGoa11cA",
        "outputId": "d29580d9-a045-4584-de10-2fc7b16f9f5f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-17 17:34:27--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-12-17 17:34:27 (20.1 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MyMultiHeadAttention(nn.Module):\n",
        "    def __init__(self, my_d_model, my_n_heads):\n",
        "        super(MyMultiHeadAttention, self).__init__()\n",
        "        self.my_n_heads = my_n_heads\n",
        "        self.my_d_head = my_d_model // my_n_heads\n",
        "\n",
        "        self.W_q = nn.Linear(my_d_model, my_d_model)\n",
        "        self.W_k = nn.Linear(my_d_model, my_d_model)\n",
        "        self.W_v = nn.Linear(my_d_model, my_d_model)\n",
        "        self.W_o = nn.Linear(my_d_model, my_d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        Q = self.W_q(x)\n",
        "        K = self.W_k(x)\n",
        "        V = self.W_v(x)\n",
        "\n",
        "        Q = Q.view(Q.size(0), -1, self.my_n_heads, self.my_d_head).transpose(1, 2)\n",
        "        K = K.view(K.size(0), -1, self.my_n_heads, self.my_d_head).transpose(1, 2)\n",
        "        V = V.view(V.size(0), -1, self.my_n_heads, self.my_d_head).transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.my_d_head ** 0.5)\n",
        "        attn_weights = nn.functional.softmax(scores, dim=-1)\n",
        "\n",
        "        out = torch.matmul(attn_weights, V).transpose(1, 2).contiguous()\n",
        "        out = out.view(out.size(0), -1, self.my_n_heads * self.my_d_head)\n",
        "\n",
        "        return self.W_o(out)"
      ],
      "metadata": {
        "id": "xpiGNvG_2M6q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MyFeedForward(nn.Module):\n",
        "    def __init__(self, my_d_model, my_d_ff):\n",
        "        super(MyFeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(my_d_model, my_d_ff)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.linear2 = nn.Linear(my_d_ff, my_d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.functional.relu(self.linear1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "9hd6GhCm2Q1j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyTransformerBlock(nn.Module):\n",
        "    def __init__(self, my_d_model, my_n_heads, my_d_ff):\n",
        "        super(MyTransformerBlock, self).__init__()\n",
        "        self.self_attention = MyMultiHeadAttention(my_d_model, my_n_heads)\n",
        "        self.feed_forward = MyFeedForward(my_d_model, my_d_ff)\n",
        "        self.layer_norm1 = nn.LayerNorm(my_d_model)\n",
        "        self.layer_norm2 = nn.LayerNorm(my_d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_output = self.self_attention(x)\n",
        "        x = x + attn_output\n",
        "        x = self.layer_norm1(x)\n",
        "\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = x + ff_output\n",
        "        x = self.layer_norm2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "lv4dI_td2SuD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyGPT2Small(nn.Module):\n",
        "    def __init__(self, my_vocab_size, my_d_model=768, my_n_heads=12, my_n_layers=12):\n",
        "        super(MyGPT2Small, self).__init__()\n",
        "        self.my_d_model = my_d_model\n",
        "        self.my_n_heads = my_n_heads\n",
        "        self.my_n_layers = my_n_layers\n",
        "        self.my_vocab_size = my_vocab_size\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(my_vocab_size, my_n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(my_block_size, my_n_embd)\n",
        "        self.transformer_blocks = nn.Sequential(*[MyTransformerBlock(my_d_model, my_n_heads, 512) for _ in range(my_n_layers)])\n",
        "        self.linear = nn.Linear(my_d_model, my_vocab_size)\n",
        "\n",
        "    def forward(self, my_idx, targets=None):\n",
        "        B, T = my_idx.shape\n",
        "        tok_emb = self.token_embedding_table(my_idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=my_device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.transformer_blocks(x)\n",
        "        logits = self.linear(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = nn.functional.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, my_idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            my_idx_cond = my_idx[:, -my_block_size:]\n",
        "            logits, loss = self(my_idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = nn.functional.softmax(logits, dim=-1)\n",
        "            my_idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            my_idx = torch.cat((my_idx, my_idx_next), dim=1)\n",
        "        return my_idx"
      ],
      "metadata": {
        "id": "tNDCkqJt2ZCV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_batch_size = 16\n",
        "my_block_size = 32\n",
        "my_max_iters = 20\n",
        "my_eval_interval = 100\n",
        "my_learning_rate = 1e-3\n",
        "my_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "my_eval_iters = 200\n",
        "my_n_embd = 464\n",
        "my_n_head = 16\n",
        "my_n_layer = 48\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as my_f:\n",
        "    my_text = my_f.read()\n",
        "\n",
        "my_chars = sorted(list(set(my_text)))\n",
        "my_vocab_size = len(my_chars)\n",
        "my_stoi = {ch: i for i, ch in enumerate(my_chars)}\n",
        "my_itos = {i: ch for i, ch in enumerate(my_chars)}\n",
        "my_encode = lambda s: [my_stoi[c] for c in s]\n",
        "my_decode = lambda l: ''.join([my_itos[i] for i in l])\n",
        "\n",
        "my_data = torch.tensor(my_encode(my_text), dtype=torch.long)\n",
        "my_n = int(0.9 * len(my_data))\n",
        "my_train_data = my_data[:my_n]\n",
        "my_val_data = my_data[my_n:]\n",
        "\n",
        "def get_my_batch(split):\n",
        "    my_data = my_train_data if split == 'train' else my_val_data\n",
        "    ix = torch.randint(len(my_data) - my_block_size, (my_batch_size,))\n",
        "    x = torch.stack([my_data[i:i + my_block_size] for i in ix])\n",
        "    y = torch.stack([my_data[i + 1:i + my_block_size + 1] for i in ix])\n",
        "    x, y = x.to(my_device), y.to(my_device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_my_loss():\n",
        "    out = {}\n",
        "    losses = {}\n",
        "    for split in ['train', 'val']:\n",
        "        for k in range(my_eval_iters):\n",
        "            x, y = get_my_batch(split)\n",
        "\n",
        "            logits, loss = my_model(x, y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = sum(losses.values()) / len(losses)\n",
        "    return out"
      ],
      "metadata": {
        "id": "cgJIKdpa2eKk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "my_save_path = 'my_gpt2_task1.pth'\n",
        "\n",
        "if os.path.isfile(my_save_path):\n",
        "    my_model = MyGPT2Small(my_vocab_size, my_n_embd, my_n_head, my_n_layer)\n",
        "    my_model.load_state_dict(torch.load(my_save_path))\n",
        "    my_model = my_model.to(my_device)\n",
        "    my_model.eval()\n",
        "    print(f\"My Model loaded from {my_save_path}\")\n",
        "    print(sum(p.numel() for p in my_model.parameters()) / 1e6, 'M parameters')\n",
        "else:\n",
        "    my_model = MyGPT2Small(my_vocab_size, my_n_embd, my_n_head, my_n_layer)\n",
        "    my_model = my_model.to(my_device)\n",
        "    print(sum(p.numel() for p in my_model.parameters()) / 1e6, 'M parameters')\n",
        "\n",
        "    my_optimizer = torch.optim.AdamW(my_model.parameters(), lr=my_learning_rate)\n",
        "\n",
        "    for my_iter in range(my_max_iters):\n",
        "        if my_iter % my_eval_interval == 0 or my_iter == my_max_iters - 1:\n",
        "            my_losses = estimate_my_loss()\n",
        "            print(f\"step {my_iter}: train loss {my_losses['train']:.4f}, val loss {my_losses['val']:.4f}\")\n",
        "\n",
        "        my_xb, my_yb = get_my_batch('train')\n",
        "\n",
        "        my_logits, my_loss = my_model(my_xb, my_yb)\n",
        "        my_optimizer.zero_grad(set_to_none=True)\n",
        "        my_loss.backward()\n",
        "        my_optimizer.step()\n",
        "\n",
        "    torch.save(my_model.state_dict(), my_save_path)\n",
        "    print(f\"My Model saved to {my_save_path}\")\n",
        "\n",
        "my_model.eval()\n",
        "\n",
        "my_context = torch.zeros((1, 1), dtype=torch.long, device=my_device)\n",
        "my_generated_text = my_decode(my_model.generate(my_context, max_new_tokens=2000)[0].tolist())\n",
        "print(my_generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNwI24Tu2h_d",
        "outputId": "d70e844d-9fd6-41e9-dadc-a61647f4c2ff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64.443617 M parameters\n",
            "step 0: train loss 4.3992, val loss 4.4008\n",
            "step 19: train loss 3.3856, val loss 3.4188\n",
            "My Model saved to my_gpt2_task1.pth\n",
            "\n",
            " NeL e,i ih lCn  Cu:ip'utose,n!asri iuWlnmdaaoimer\n",
            "nil tho\n",
            "neEg'OrL T    noSedudndaa. a  oeo n.sLc\n",
            "diu!t' s ow;, Cn ouAe g  pn  nKClh Msic ainetMttuTaf i'gr;nnlt ;  hyE l  .lss e hnlSs yrE'pNosvlhm\n",
            "ea on,entn t  ymgn ptI d c,'unThO ,v.\n",
            "t  \n",
            " bllyfW! hhOioe oeed l ne\n",
            "\n",
            "dndet a elnt\n",
            "HseNi E'nn nJtlndio c! e we te    l  rmekNu eaWe   eHnIF't     e\n",
            "f  ,   \n",
            "trnt r\n",
            "  wos, ibyt Sf  poycT Lonrni h hab  kd  \n",
            "n dbaoe andTf,oy\n",
            "\n",
            "!i k ooh'uea  rdBt\n",
            "t h :a ,n : etikrin;n   hM:\n",
            "te eogE.s n wIeina Pler eollTKEu uT oTrwtot ptlr \n",
            "m  c, u eh'l FnOne nnugonn l\n",
            ":rn\n",
            "o rr aipmr tmrI 'aiyrN  r T  eico aaDu'Vsr'  n ug gnt a o Eehiearo \n",
            "n tnesee,,han s, i  gantnWceuobnWy?, s, ,;,fatsuehnpnhy ihr Lguona  n ,MTu h eeodLysahfUadhbs Nn siatn    EB, dhrUu:rS  \n",
            "\n",
            "E\n",
            " nnlc lht !\n",
            "uaRsenoSE s:WLlTl rr'tngip am noslnTaeve E ic\n",
            ".h  !ESenWl,S oep   be t  trh ab  Ti'fetE nTu \n",
            "gnnnntvT n necr eerOctSeseaoUi ndb O\n",
            "hopu\n",
            "e:Ft Lin  n,na \n",
            "udL oRnnt Ka Ufh!ch  brhT ttp e  sitshe spbheweiuenan b oc pT nlLmt n ,n t nn;eoutbce tefmy yIVn rsc\n",
            "a ntpo rIhvttMBsr t\n",
            "orB'  t\n",
            "T:tseb  nt  eanno a.h se df  T n\n",
            " pp    ow\n",
            "liw utsolSNe to nOat NeTsdtLSeed nrr, ppnntEaepnn;.f  i sntew  !a i Shntn\n",
            " st nfNlusebl\n",
            "p\n",
            "tnca ws rshl\n",
            "  ls u ohlril ee,An l  cesdlenfcIsSnh UuhI , gfoh,e  uldeniYvT hpag Lr eounWbdd  \n",
            " ee a utro\n",
            "h ntyraT!sup nh \n",
            "dhnnAho  , f En\n",
            "  .ctbnl unndrnmafa Hu:r :Pg,Nv s  Ytn!uu tlt ln,i,cnyn'fi SnMcYosb ee dosap:tanCRo\n",
            "tn u, oien: n  \n",
            "na eimg ssgil n!if,ga  tfp nI ktt , iua hdttna\n",
            "rLk    uEhnat ormvi lRTnhoenltktegs   EpnN, :i I   fo  s?bg lbMikye  e\n",
            "otfuyet ncwe ,BoHgyne\n",
            "h' on us cae\n",
            " nt\n",
            "ab y\n",
            " nofeVosiv,iat  Mt otrsunn l mooon  nk,fvenonnrp NshTomiehoioNe u RR pte\n",
            "ohun  dpi \n",
            "' soocr lIFcgSde - nsebM\n",
            "nAw Uu o'ibd\n",
            "Ewvmeb  teaupm;  OlfsIWeasftcUts iea anlrp e loin : g :nsnlr  Tru \n",
            "TlTlhm'n po n,\n",
            "'pIpAt lcnn afF,  nvon e i hrtatunTTu \n",
            "gnTCpnn   reer\n",
            "rtoOh eup : '  bhni inAa a rSt:\n",
            "otI \n",
            "\n",
            "Tt T yusBa .msute!\n",
            "agnmkatlbso\n",
            "h rru ynnUIUiW ,ano   'riiEsnnctLouhnuenTfcerIn  N\n",
            "  nin! dJnq rna   nccii i Uete Nsit R, ClMVunele;ri  tc\n"
          ]
        }
      ]
    }
  ]
}